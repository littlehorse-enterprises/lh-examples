from typing import Any

from langchain.schema import HumanMessage, SystemMessage
from model import llm
from utils.constants import TaskDefNames
from utils.logger import logger
from utils.worker_registry import worker


# @worker(TaskDefNames.ORCHESTRATE_TOPICS)
async def orchestrate_topics(prompt: str) -> str:
    logger.info(f"Orchestrating topics for prompt: {prompt}")
    chat = llm.with_structured_output({
        "name": "research_orchestration",
        "description": "A number of prompts for other AI LLM workers that represent specific topics to research to form a full business plan.",
        "parameters": {
            "type": "object",
            "properties": {
                "topics": {
                    "type": "array",
                    "items": {
                        "type": "string",
                        "description": "A prompt for an AI LLM worker that will research the specific topic and compile information for."
                    }
                }
            },
            "required": ["topics"]
        }
    })

    response = chat.invoke([
        SystemMessage(
            content="You are an experienced and helpful AI orchesrator who will create a series of prompts for other AI LLM workers that represent specific topics to research to form a full business plan."),
        HumanMessage(content=prompt)
    ])

    return response


@worker(TaskDefNames.FETCH_CUSTOMER_CRM_DATA)
async def fetch_customer_crm_data(user_id: str) -> dict[str, Any]:
    # In a real world scenario, this would be a call to a CRM API like Salesforce or Hubspot
    return {
        "customerId": "cust_834792",
        "firstName": "Emily",
        "lastName": "Tran",
        "email": "emily.tran@futurefintech.com",
        "phone": "+1-415-555-0198",
        "company": "Future FinTech",
        "jobTitle": "Lead Platform Engineer",
        "industry": "Financial Services",
        "location": {
            "city": "San Francisco",
            "state": "CA",
            "country": "USA"
        },
        "accountStatus": "Active",
        "lastContacted": "2025-03-15T10:34:00Z",
        "lastActivity": "2025-04-10T14:23:00Z",
        "lifecycleStage": "Customer",
        "dealSize": 150000,
        "leadSource": "Webinar - Microservice Scaling",
        "interests": ["Workflow Automation", "Observability", "Microservices"],
        "notes": "Interested in improving internal microservice reliability. Mentioned a pain point around manual escalation handling."
    }

@worker(TaskDefNames.GENERATE_EMAIL)
async def generate_email(customer_data: dict[str, Any], user_instructions: str, ai_evaluator_feedback: str, previous_email: str) -> str:
    chat = llm.with_structured_output({
        "name": "email",
        "description": "A LLM that will generate an email to the customer based on the CRM data and the users instructions and any feedback the AI Evaluator has provided.",
        "parameters": {
            "type": "object",
            "properties": {
                "to": {
                    "type": "string",
                    "description": "The email address of the customer."
                },
                "subject": {
                    "type": "string",
                    "description": "The subject of the email."
                },
                "body": {
                    "type": "string",
                    "description": "The body of the email."
                }
            },
            "required": ["to", "subject", "body"]
        }
    })
    response = chat.invoke([
        SystemMessage(content="You are an experienced email writer who will write an email to a customer based on the CRM data and the users instructions and any feedback the AI Evaluator has provided."),
        HumanMessage(content=f"Customer CRM Data: {customer_data}\nUser Instructions: {user_instructions}\nAI Evaluator Feedback: {ai_evaluator_feedback}\nPrevious Email: {previous_email}")
    ])

    return response["email"]

@worker(TaskDefNames.APPROVE_EMAIL)
async def approve_email(email: str) -> bool:
    chat = llm.with_structured_output({
        "name": "feedback",
        "description": "Feedback that will be sent back to an AI LLM worker that is generating a response.",
        "parameters": {
            "type": "object",
            "properties": {
                "feedback": {
                    "type": "string",
                    "description": "Feedback that will be sent back to an AI LLM worker that is generating a response."
                }
            },
            "required": ["feedback"]
        }
    })
    response = chat.invoke([
        SystemMessage(
            content="You will recieve an email from another LLM. Your job is to decide if the text generated by the other LLM accomplishes the task it was given. If it does, return an empty string. If it does not, return feedback on how to improve the text. You are in a loop with the other LLM, so you will need to keep generating feedback until the other LLM returns text that you are happy with."),
        HumanMessage(content=email)
    ])

    return response["feedback"]




